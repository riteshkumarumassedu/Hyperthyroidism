{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "using device: cpu\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import os\n",
    "from load_data import *\n",
    "from skimage import io, transform\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data import sampler\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "import torchvision.datasets as dset\n",
    "from torchvision import transforms, utils\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as model\n",
    "import graphviz\n",
    "import numpy as np\n",
    "from graphviz import Digraph\n",
    "import glob\n",
    "from cv2 import resize as resize\n",
    "from numpy import zeros, newaxis\n",
    "import torchvision\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "USE_GPU = True\n",
    "\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Constant to control how frequently we print train loss\n",
    "print_every = 20\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sliced_dataset(Dataset):\n",
    "    \"\"\"Sliced_dataset\"\"\"\n",
    "\n",
    "    def __init__(self, txt_file, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.images_dict={}\n",
    "        self.images_id_dict= {}\n",
    "        self.image_id=0\n",
    "        self.class_map = {\"MCI\":0, \"AD\" : 1, \"CN\" : 2}\n",
    "        \n",
    "        # get the subjects from the directory\n",
    "        self.subjects_dirs = pd.read_csv(txt_file)\n",
    "        self.subjects_dirs= self.subjects_dirs.values.tolist()\n",
    "        \n",
    "        from random import shuffle\n",
    "        shuffle(self.subjects_dirs)\n",
    "        for one_subject_dir in self.subjects_dirs:\n",
    "            # get subject class label\n",
    "            subject_label = one_subject_dir[0].split('_')[-1]\n",
    "            # get all the images from the subject dir\n",
    "            file_reg = one_subject_dir[0]+\"/*.tiff\"\n",
    "            sub_images = glob.glob(file_reg)\n",
    "            \n",
    "            # add the image path and class label to dict\n",
    "            for one_image in sub_images:\n",
    "                self.images_id_dict[self.image_id] = one_image\n",
    "                self.images_dict[one_image] = self.class_map[subject_label]\n",
    "                self.image_id += 1\n",
    "        \n",
    "        print(len(self.images_dict))\n",
    "        print(len(self.images_id_dict))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_dict)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # read the iamge\n",
    "        image = io.imread(self.images_id_dict[idx])\n",
    "        \n",
    "        # image resizing as per Joie\n",
    "        image = np.array(image)\n",
    "        image_hat = image[20:380,50:400]\n",
    "        image = resize(image_hat[:,:,1],(224,224))\n",
    "        image= image[newaxis,:, :]\n",
    "        label = self.images_dict[self.images_id_dict[idx]]\n",
    "        sample = (image,label)\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### get the images data set\n",
    "# ### and divide them into training and test sets\n",
    "image_dir_file=\"/Users/riteshkumar/Desktop/coronal_skullstrip/subjectID_label_match.txt\"\n",
    "training_loss={}\n",
    "validation_loss= []\n",
    "test_loss=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1788\n",
      "1788\n"
     ]
    }
   ],
   "source": [
    "NUM_TRAIN=1400\n",
    "batch_size=100\n",
    "num_val=100\n",
    "\n",
    "\n",
    "FD= Sliced_dataset(txt_file=image_dir_file)\n",
    "train_loader = DataLoader(FD, batch_size=batch_size, sampler=sampler.SubsetRandomSampler(range(800,1700)))\n",
    "\n",
    "loader_val = DataLoader(FD, batch_size=batch_size, \n",
    "                        sampler=sampler.SubsetRandomSampler(range(300, 500)))\n",
    "\n",
    "test_loader = DataLoader(FD, batch_size=batch_size, sampler=sampler.SubsetRandomSampler(range(1,300)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val(model, optimizer, epochs=1):\n",
    "    \"\"\"\n",
    "    Train a model on CIFAR-10 using the PyTorch Module API.\n",
    "    \n",
    "    Inputs:\n",
    "    - model: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "    \n",
    "    Returns: Nothing, but prints model accuracies during training.\n",
    "    \"\"\"\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    for e in range(epochs):\n",
    "        for t, (x, y) in enumerate(train_loader):\n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "\n",
    "            if t % print_every == 0:\n",
    "                print('Iteration %d, loss = %.4f' % (t, loss.item()))\n",
    "                check_accuracy_part34(loader_val, model)\n",
    "                print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy_part34(loader, model):\n",
    "#     if loader.dataset.train:\n",
    "#         print('Checking accuracy on validation set')\n",
    "#     else:\n",
    "#         print('Checking accuracy on test set')   \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(x):\n",
    "    N = x.shape[0] # read in N, C, H, W\n",
    "#     print(x.shape)\n",
    "    return x.view(N, -1)\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return flatten(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 1.0979\n",
      "Got 57 / 200 correct (28.50)\n",
      "\n",
      "Iteration 0, loss = 1.0649\n",
      "Got 102 / 200 correct (51.00)\n",
      "\n",
      "Iteration 0, loss = 1.0043\n",
      "Got 104 / 200 correct (52.00)\n",
      "\n",
      "Iteration 0, loss = 1.0002\n",
      "Got 110 / 200 correct (55.00)\n",
      "\n",
      "Iteration 0, loss = 0.8191\n",
      "Got 92 / 200 correct (46.00)\n",
      "\n",
      "Iteration 0, loss = 0.6141\n",
      "Got 110 / 200 correct (55.00)\n",
      "\n",
      "Iteration 0, loss = 0.6879\n",
      "Got 106 / 200 correct (53.00)\n",
      "\n",
      "Iteration 0, loss = 0.3983\n",
      "Got 131 / 200 correct (65.50)\n",
      "\n",
      "Iteration 0, loss = 0.2646\n",
      "Got 144 / 200 correct (72.00)\n",
      "\n",
      "Iteration 0, loss = 0.1560\n",
      "Got 133 / 200 correct (66.50)\n",
      "\n",
      "Iteration 0, loss = 0.1276\n",
      "Got 131 / 200 correct (65.50)\n",
      "\n",
      "Iteration 0, loss = 0.1232\n",
      "Got 160 / 200 correct (80.00)\n",
      "\n",
      "Iteration 0, loss = 0.0229\n",
      "Got 150 / 200 correct (75.00)\n",
      "\n",
      "Iteration 0, loss = 0.0180\n",
      "Got 156 / 200 correct (78.00)\n",
      "\n",
      "Iteration 0, loss = 0.0052\n",
      "Got 146 / 200 correct (73.00)\n",
      "\n",
      "Iteration 0, loss = 0.0034\n",
      "Got 160 / 200 correct (80.00)\n",
      "\n",
      "Iteration 0, loss = 0.0029\n",
      "Got 162 / 200 correct (81.00)\n",
      "\n",
      "Iteration 0, loss = 0.0020\n",
      "Got 160 / 200 correct (80.00)\n",
      "\n",
      "Iteration 0, loss = 0.0013\n",
      "Got 162 / 200 correct (81.00)\n",
      "\n",
      "Iteration 0, loss = 0.0010\n",
      "Got 161 / 200 correct (80.50)\n",
      "\n",
      "Iteration 0, loss = 0.0011\n",
      "Got 161 / 200 correct (80.50)\n",
      "\n",
      "Iteration 0, loss = 0.0007\n",
      "Got 161 / 200 correct (80.50)\n",
      "\n",
      "Iteration 0, loss = 0.0008\n",
      "Got 162 / 200 correct (81.00)\n",
      "\n",
      "Iteration 0, loss = 0.0008\n",
      "Got 162 / 200 correct (81.00)\n",
      "\n",
      "Iteration 0, loss = 0.0007\n",
      "Got 162 / 200 correct (81.00)\n",
      "\n",
      "Iteration 0, loss = 0.0006\n",
      "Got 162 / 200 correct (81.00)\n",
      "\n",
      "Iteration 0, loss = 0.0006\n",
      "Got 163 / 200 correct (81.50)\n",
      "\n",
      "Iteration 0, loss = 0.0005\n",
      "Got 163 / 200 correct (81.50)\n",
      "\n",
      "Iteration 0, loss = 0.0003\n",
      "Got 162 / 200 correct (81.00)\n",
      "\n",
      "Iteration 0, loss = 0.0005\n",
      "Got 163 / 200 correct (81.50)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = None\n",
    "optimizer = None\n",
    "\n",
    "\n",
    "\n",
    "# creating a 3-layer convonet\n",
    "\n",
    "layer1 = nn.Sequential( nn.Conv2d(1, 5, kernel_size=5, stride=1),nn.ReLU(), nn.MaxPool2d(2) )\n",
    "\n",
    "layer2 = nn.Sequential(nn.Conv2d(5, 10, kernel_size=3, stride=1), nn.ReLU(), nn.MaxPool2d(2) )\n",
    "\n",
    "layer3 = nn.Sequential( nn.Conv2d(10, 20, kernel_size=3, stride=1), nn.ReLU(), nn.MaxPool2d(2) )\n",
    "\n",
    "layer4 = nn.Sequential( nn.Conv2d(20, 30, kernel_size=3, stride=1), nn.ReLU(), nn.MaxPool2d(2) )\n",
    "\n",
    "layer5 = nn.Sequential( nn.Conv2d(30, 35, kernel_size=3, stride=1), nn.ReLU(), nn.MaxPool2d(2) )\n",
    "\n",
    "layer6 = nn.Sequential( nn.Conv2d(35, 40, kernel_size=3, padding=1, stride=1), nn.ReLU() )\n",
    "\n",
    "fc = nn.Linear(40*5*5, 200)\n",
    "fc1 = nn.Linear(200, 3)\n",
    "\n",
    "model = nn.Sequential( layer1,layer2, layer3, layer4, layer5, layer6, Flatten(),fc, fc1)\n",
    "\n",
    "print_every = 10000\n",
    "\n",
    "learning_rate = 8e-4\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             \n",
    "################################################################################\n",
    "\n",
    "# You should get at least 70% accuracy\n",
    "train_val(model, optimizer, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 246 / 299 correct (82.27)\n"
     ]
    }
   ],
   "source": [
    "best_model = model\n",
    "check_accuracy_part34(test_loader, best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
